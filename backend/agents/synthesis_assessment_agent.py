"""
Synthesis & Assessment LLM Agent - T√°c nh√¢n t·ªïng h·ª£p v√† ƒë√°nh gi√° trong h·ªá th·ªëng MCTS
"""

import json
import re
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime

from .base_agent import BaseAgent, AgentInput, AgentOutput
from config import MCTSConfig, AgentWeights, EVALUATION_CRITERIA

logger = logging.getLogger(__name__)

@dataclass
class SynthesisTask:
    """Task cho vi·ªác t·ªïng h·ª£p v√† ƒë√°nh gi√°"""
    primary_output: str
    ct_feedback: Optional[str] = None
    ae_feedback: Optional[str] = None
    esv_results: Optional[Dict[str, Any]] = None
    phase: str = "analysis"  # "analysis" ho·∫∑c "ideas"
    iteration: int = 1

@dataclass
class LoopDecision:
    """Quy·∫øt ƒë·ªãnh cho v√≤ng l·∫∑p ti·∫øp theo"""
    action: str  # "continue", "stop", "user_checkpoint"
    reasoning: str
    next_round_instructions: Optional[Dict[str, Any]] = None
    user_checkpoint_info: Optional[Dict[str, Any]] = None
    quality_metrics: Optional[Dict[str, Any]] = None

@dataclass
class QualityScore:
    """ƒêi·ªÉm ch·∫•t l∆∞·ª£ng chi ti·∫øt"""
    criterion: str
    raw_score: float
    weight: float
    weighted_score: float
    red_flag: bool
    reasoning: str

class SynthesisAssessmentAgent(BaseAgent):
    """
    Synthesis & Assessment Agent - Gi√°m ƒë·ªëc d·ª± √°n, ng∆∞·ªùi ƒë∆∞a ra quy·∫øt ƒë·ªãnh
    """
    
    def __init__(self, config: MCTSConfig, llm_client):
        super().__init__("synthesis", config, llm_client)
        self.weights = config.weights
        self.quality_threshold = config.quality_threshold
        self.improvement_threshold = config.improvement_threshold
        self.red_flag_threshold = config.red_flag_threshold
        
        # Tracking metrics
        self.iteration_scores: List[float] = []
        self.iteration_decisions: List[LoopDecision] = []
        
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """
        Main processing method cho Synthesis Assessment Agent
        """
        try:
            # Validate input
            if not await self.validate_input(agent_input):
                return self._create_agent_output(
                    content="",
                    success=False,
                    agent_input=agent_input,
                    error="Invalid input"
                )
            
            # Process synthesis task
            if isinstance(agent_input.data, SynthesisTask):
                return await self._process_synthesis_task(agent_input)
            else:
                # Fallback: treat as raw content for basic assessment
                return await self._process_raw_content(agent_input)
                
        except Exception as e:
            logger.error(f"Error in SynthesisAssessmentAgent.process: {str(e)}")
            return self._create_agent_output(
                content="",
                success=False,
                agent_input=agent_input,
                error=str(e)
            )
    
    async def _process_synthesis_task(self, agent_input: AgentInput) -> AgentOutput:
        """X·ª≠ l√Ω task t·ªïng h·ª£p v√† ƒë√°nh gi√°"""
        task: SynthesisTask = agent_input.data
        
        # T·∫°o prompt t·ªïng h·ª£p
        synthesis_prompt = self._build_synthesis_prompt(task, agent_input)
        
        # G·ªçi LLM v·ªõi temperature th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o objectivity
        llm_response = await self._make_llm_call(
            user_message=synthesis_prompt,
            temperature=0.1,  # R·∫•t th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o consistent decision making
            use_conversation_history=task.iteration > 1
        )
        
        if not llm_response.success:
            return self._create_agent_output(
                content="",
                success=False,
                agent_input=agent_input,
                error=llm_response.error
            )
        
        # Parse quality scores v√† metrics
        quality_scores = await self._parse_quality_scores(
            llm_response.content, task.phase
        )
        
        # Calculate overall score
        overall_score = self._calculate_overall_score(quality_scores)
        
        # Determine loop decision
        loop_decision = await self._determine_loop_decision(
            task, overall_score, quality_scores, agent_input
        )
        
        # Update tracking
        self.iteration_scores.append(overall_score)
        self.iteration_decisions.append(loop_decision)
        
        # Post-process output
        processed_output = await self.post_process_output(llm_response, agent_input)
        
        return self._create_agent_output(
            content=processed_output,
            success=True,
            agent_input=agent_input,
            metadata={
                "phase": task.phase,
                "overall_score": overall_score,
                "quality_scores": [score.__dict__ for score in quality_scores],
                "loop_decision": loop_decision.__dict__,
                "esv_activated": task.esv_results is not None,
                "token_usage": llm_response.usage
            }
        )
    
    async def _process_raw_content(self, agent_input: AgentInput) -> AgentOutput:
        """X·ª≠ l√Ω raw content - fallback method"""
        content = str(agent_input.data)
        
        # Create basic synthesis task
        synthesis_task = SynthesisTask(
            primary_output=content,
            phase="analysis",  # Default to analysis
            iteration=agent_input.iteration
        )
        
        # Create new agent input
        new_input = AgentInput(
            data=synthesis_task,
            context=agent_input.context,
            metadata=agent_input.metadata,
            iteration=agent_input.iteration
        )
        
        return await self._process_synthesis_task(new_input)
    
    def _build_synthesis_prompt(self, 
                               task: SynthesisTask,
                               agent_input: AgentInput) -> str:
        """X√¢y d·ª±ng prompt cho t·ªïng h·ª£p v√† ƒë√°nh gi√°"""
        
        # Build input analysis section
        input_analysis = self._build_input_analysis_section(task)
        
        # Build scoring framework
        scoring_framework = self._build_scoring_framework(task.phase)
        
        # Build ESV results section
        esv_section = self._build_esv_section(task.esv_results) if task.esv_results else ""
        
        # Build decision logic
        decision_logic = self._build_decision_logic_section()

        # Diversity analysis (n·∫øu c√≥)
        diversity_context = ""
        idea_diversity = agent_input.context.get("idea_diversity_analysis")
        if idea_diversity:
            diversity_context = f"""
## DIVERSITY CONTEXT (√ù t∆∞·ªüng)

- Ideas: {idea_diversity.get('ideas_count')}
- Diversity Score: {idea_diversity.get('diversity_score')}
- Unique Audiences: {idea_diversity.get('unique_audiences')}
- Unique Business Models: {idea_diversity.get('unique_business_models')}
- Unique Techs: {idea_diversity.get('unique_techs')}
- Duplicates: {len(idea_diversity.get('duplicates', []))}
"""
        
        prompt = f"""
# NHI·ªÜM V·ª§ T·ªîNG H·ª¢P & ƒê√ÅNH GI√Å - V√íNG {task.iteration}

## TH√îNG TIN ƒê·∫¶U V√ÄO

### Phase hi·ªán t·∫°i: {task.phase.upper()}

{input_analysis}

{esv_section}

{diversity_context}

## FRAMEWORK ƒêI·ªÇM S·ªê

{scoring_framework}

## LOGIC QUY·∫æT ƒê·ªäNH

{decision_logic}

## Y√äU C·∫¶U C·ª§ TH·ªÇ

{"### ƒê√ÇY L√Ä V√íNG ƒê√ÅNH GI√Å S·ªê " + str(task.iteration) + " - H√ÉY SO S√ÅNH V·ªöI C√ÅC V√íNG TR∆Ø·ªöC V√Ä T√çNH IMPROVEMENT RATE" if task.iteration > 1 else ""}

Th·ª±c hi·ªán t·ªïng h·ª£p v√† ƒë√°nh gi√° TO√ÄN DI·ªÜN theo ƒë√∫ng format trong system prompt.

**Nhi·ªám v·ª• ch√≠nh:**
1. **PH√ÇN T√çCH INPUT** - ƒê√°nh gi√° output t·ª´ t·∫•t c·∫£ c√°c agents
2. **T√çNH ƒêI·ªÇM CH·∫§T L∆Ø·ª¢NG** - √Åp d·ª•ng h·ªá th·ªëng scoring c√≥ tr·ªçng s·ªë
3. **X√ÅC ƒê·ªäNH C·ªú ƒê·ªé** - Identify critical issues
4. **GI·∫¢I QUY·∫æT CONFLICTS** - Reconcile disagreements gi·ªØa agents
5. **QUY·∫æT ƒê·ªäNH V√íNG L·∫∂P** - Continue/Stop/User_Checkpoint
6. **ƒê∆ØA RA H∆Ø·ªöNG D·∫™N** - Instructions for next round

**Tr·ªçng s·ªë hi·ªán t·∫°i:**
{self._format_weights()}

**Ng∆∞·ª°ng quy·∫øt ƒë·ªãnh:**
- Quality threshold: {self.quality_threshold}/10
- Improvement threshold: {self.improvement_threshold}% (5%)
- Red flag threshold: <{self.red_flag_threshold}/10

**Ch√∫ √Ω ƒë·∫∑c bi·ªát:**
- ƒê√°nh gi√° objective v√† data-driven
- Balance gi·ªØa quality v√† efficiency
- Consider user experience v√† feedback
- Focus on actionable outcomes
- Maintain high standards nh∆∞ng realistic

## NG·ªÆ C·∫¢NH B·ªî SUNG
{json.dumps(agent_input.context, ensure_ascii=False, indent=2) if agent_input.context else "Kh√¥ng c√≥ ng·ªØ c·∫£nh b·ªï sung"}

B·∫Øt ƒë·∫ßu t·ªïng h·ª£p v√† ƒë√°nh gi√° ngay b√¢y gi·ªù:
"""
        
        return prompt.strip()
    
    def _build_input_analysis_section(self, task: SynthesisTask) -> str:
        """X√¢y d·ª±ng section ph√¢n t√≠ch input"""
        
        sections = []
        
        # Primary output
        sections.append(f"""
### üìÑ OUTPUT T·ª™ PRIMARY LLM
**ƒê·ªô d√†i:** {len(task.primary_output)} k√Ω t·ª±
**Preview:**
```
{task.primary_output[:500]}{'...' if len(task.primary_output) > 500 else ''}
```
""")
        
        # CT feedback
        if task.ct_feedback:
            sections.append(f"""
### üß† PH·∫¢N H·ªíI T·ª™ CT-LLM (T∆∞ duy Ph·∫£n bi·ªán)
**ƒê·ªô d√†i:** {len(task.ct_feedback)} k√Ω t·ª±
**Preview:**
```
{task.ct_feedback[:500]}{'...' if len(task.ct_feedback) > 500 else ''}
```
""")
        else:
            sections.append("### üß† PH·∫¢N H·ªíI T·ª™ CT-LLM: Kh√¥ng c√≥")
        
        # AE feedback
        if task.ae_feedback:
            sections.append(f"""
### ‚öîÔ∏è PH·∫¢N H·ªíI T·ª™ AE-LLM (Chuy√™n gia ƒê·ªëi kh√°ng)
**ƒê·ªô d√†i:** {len(task.ae_feedback)} k√Ω t·ª±  
**Preview:**
```
{task.ae_feedback[:500]}{'...' if len(task.ae_feedback) > 500 else ''}
```
""")
        else:
            sections.append("### ‚öîÔ∏è PH·∫¢N H·ªíI T·ª™ AE-LLM: Kh√¥ng c√≥")
        
        return "\n".join(sections)
    
    def _build_esv_section(self, esv_results: Dict[str, Any]) -> str:
        """X√¢y d·ª±ng section ESV results"""
        
        if not esv_results:
            return ""
        
        queries = esv_results.get("queries", [])
        summary = esv_results.get("summary", {})
        
        section = f"""
## üåê K·∫æT QU·∫¢ X√ÅC TH·ª∞C NGO√ÄI (ESV)

### Queries ƒë√£ th·ª±c hi·ªán:
{chr(10).join(f"- {query}" for query in queries)}

### T√≥m t·∫Øt validation:
- Confirmed claims: {summary.get('confirmed', 0)}
- Refuted claims: {summary.get('refuted', 0)}  
- New information: {summary.get('new_info', 'Kh√¥ng c√≥')}

### Impact l√™n ƒë√°nh gi√°:
{esv_results.get('impact', 'C·∫ßn ph√¢n t√≠ch trong synthesis')}
"""
        
        return section
    
    def _build_scoring_framework(self, phase: str) -> str:
        """X√¢y d·ª±ng framework ƒëi·ªÉm s·ªë"""
        
        criteria = EVALUATION_CRITERIA.get(phase, EVALUATION_CRITERIA["analysis"])
        weights_dict = self.weights.__dict__
        
        framework = f"""
### TI√äU CH√ç ƒê√ÅNH GI√Å CHO {phase.upper()}:

| Ti√™u ch√≠ | Tr·ªçng s·ªë | M√¥ t·∫£ |
|----------|----------|-------|"""
        
        # Map criteria to weights v√† descriptions
        criteria_mapping = {
            "tinh_logic": ("T√≠nh Logic", "Chu·ªói suy lu·∫≠n h·ª£p l√Ω, tr√°nh fallacy"),
            "toan_dien": ("T√≠nh To√†n di·ªán", "Ph·∫°m vi ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß"),
            "nhat_quan": ("T√≠nh Nh·∫•t qu√°n", "Th·ªëng nh·∫•t n·ªôi t·∫°i"),
            "bang_chung": ("Ch·∫•t l∆∞·ª£ng B·∫±ng ch·ª©ng", "ƒê·ªô tin c·∫≠y c·ªßa data"),
            "do_sau": ("ƒê·ªô S√¢u", "Chi ti·∫øt v√† hi·ªÉu bi·∫øt context"),
            "tinh_kha_thi": ("T√≠nh Kh·∫£ thi", "Kh·∫£ thi k·ªπ thu·∫≠t v√† t√†i ch√≠nh"),
            "tiem_nang_thi_truong": ("Ti·ªÅm nƒÉng Th·ªã tr∆∞·ªùng", "Market size v√† timing"),
            "tinh_sang_tao": ("T√≠nh S√°ng t·∫°o", "ƒê·ªô ƒë·ªôc ƒë√°o v√† innovation"),
            "mo_hinh_kinh_doanh": ("M√¥ h√¨nh Kinh doanh", "Revenue streams b·ªÅn v·ªØng"),
            "loi_the_canh_tranh": ("L·ª£i th·∫ø C·∫°nh tranh", "Differentiation m·∫°nh"),
            "rui_ro_ky_thuat": ("R·ªßi ro K·ªπ thu·∫≠t", "Technical complexity v√† risks"),
            "dau_tu_ban_dau": ("ƒê·∫ßu t∆∞ Ban ƒë·∫ßu", "Capital requirements")
        }
        
        for criterion in criteria:
            if criterion in criteria_mapping:
                name, desc = criteria_mapping[criterion]
                weight = getattr(self.weights, criterion, 1.0)
                framework += f"\n| {name} | {weight} | {desc} |"
        
        framework += f"""

### C√îNG TH·ª®C T√çNH ƒêI·ªÇM:
```
Weighted_Score = Œ£(Individual_Score √ó Weight) / Œ£(Weights)
Red_Flag = ANY individual score < {self.red_flag_threshold}
```
"""
        
        return framework
    
    def _build_decision_logic_section(self) -> str:
        """X√¢y d·ª±ng section logic quy·∫øt ƒë·ªãnh"""
        
        return f"""
### TI√äU CH√ç D·ª™NG:

1. **CH·∫§T L∆Ø·ª¢NG H·ªòI T·ª§:** Score ‚â• {self.quality_threshold} AND No Red Flags
2. **GI·∫¢M D·∫¶N C·∫¢I THI·ªÜN:** Improvement < {self.improvement_threshold}% for 2 rounds  
3. **GI·ªöI H·∫†N T√ÄI NGUY√äN:** ƒê·∫°t max loops
4. **CH·ªà ƒê·∫†O NG∆Ø·ªúI D√ôNG:** User decision

### K√çCH HO·∫†T USER CHECKPOINT:
- Score improvement < 10% trong 1 v√≤ng
- C√≥ Red Flags nghi√™m tr·ªçng  
- Conflict l·ªõn gi·ªØa agents
- Strategic decision c·∫ßn input
"""
    
    def _format_weights(self) -> str:
        """Format weights ƒë·ªÉ hi·ªÉn th·ªã"""
        weights_dict = self.weights.__dict__
        return "\n".join(f"- {key}: {value}" for key, value in weights_dict.items())
    
    async def _parse_quality_scores(self, 
                                  content: str, 
                                  phase: str) -> List[QualityScore]:
        """Parse quality scores t·ª´ LLM output"""
        
        scores = []
        criteria = EVALUATION_CRITERIA.get(phase, EVALUATION_CRITERIA["analysis"])
        
        try:
            # Look for scoring table in content
            table_pattern = r"\|(.*?)\|(.*?)\|(.*?)\|(.*?)\|(.*?)\|"
            matches = re.findall(table_pattern, content)
            
            for match in matches:
                if len(match) >= 4:
                    criterion_text = match[0].strip()
                    score_text = match[1].strip()
                    
                    # Extract score
                    score_match = re.search(r"(\d+)", score_text)
                    if score_match:
                        raw_score = float(score_match.group(1))
                        
                        # Find matching criterion
                        for criterion in criteria:
                            if criterion in criterion_text.lower() or criterion.replace("_", " ") in criterion_text.lower():
                                weight = getattr(self.weights, criterion, 1.0)
                                weighted_score = raw_score * weight
                                red_flag = raw_score < self.red_flag_threshold
                                
                                scores.append(QualityScore(
                                    criterion=criterion,
                                    raw_score=raw_score,
                                    weight=weight,
                                    weighted_score=weighted_score,
                                    red_flag=red_flag,
                                    reasoning=match[2].strip() if len(match) > 2 else ""
                                ))
                                break
            
            # If no scores found, create default scores
            if not scores:
                logger.warning("No quality scores found, creating defaults")
                for criterion in criteria:
                    weight = getattr(self.weights, criterion, 1.0)
                    scores.append(QualityScore(
                        criterion=criterion,
                        raw_score=5.0,  # Default middle score
                        weight=weight,
                        weighted_score=5.0 * weight,
                        red_flag=True,  # Flag as questionable
                        reasoning="Could not parse from output"
                    ))
        
        except Exception as e:
            logger.error(f"Error parsing quality scores: {str(e)}")
        
        return scores
    
    def _calculate_overall_score(self, quality_scores: List[QualityScore]) -> float:
        """T√≠nh to√°n ƒëi·ªÉm t·ªïng th·ªÉ"""
        
        if not quality_scores:
            return 0.0
        
        total_weighted = sum(score.weighted_score for score in quality_scores)
        total_weights = sum(score.weight for score in quality_scores)
        
        return total_weighted / total_weights if total_weights > 0 else 0.0
    
    async def _determine_loop_decision(self, 
                                     task: SynthesisTask,
                                     overall_score: float,
                                     quality_scores: List[QualityScore],
                                     agent_input: AgentInput) -> LoopDecision:
        """X√°c ƒë·ªãnh quy·∫øt ƒë·ªãnh cho v√≤ng l·∫∑p ti·∫øp theo"""
        
        # Check red flags
        red_flags = [score for score in quality_scores if score.red_flag]
        has_critical_red_flags = len(red_flags) > 0
        
        # Check quality threshold
        quality_achieved = overall_score >= self.quality_threshold and not has_critical_red_flags
        
        # Check improvement rate
        improvement_rate = self._calculate_improvement_rate()
        diminishing_returns = improvement_rate < self.improvement_threshold if improvement_rate is not None else False
        
        # Check resource limits (would need to be passed in context)
        max_loops = agent_input.context.get("max_loops", 10)
        resource_limit_reached = task.iteration >= max_loops
        
        # Decision logic
        if quality_achieved:
            return LoopDecision(
                action="stop",
                reasoning=f"Quality threshold achieved: {overall_score:.2f} ‚â• {self.quality_threshold}, no red flags",
                quality_metrics={
                    "overall_score": overall_score,
                    "red_flags_count": len(red_flags),
                    "improvement_rate": improvement_rate
                }
            )
        
        elif resource_limit_reached:
            return LoopDecision(
                action="stop",
                reasoning=f"Resource limit reached: {task.iteration} ‚â• {max_loops}",
                quality_metrics={
                    "overall_score": overall_score,
                    "red_flags_count": len(red_flags),
                    "improvement_rate": improvement_rate
                }
            )
        
        elif diminishing_returns and task.iteration > 2:
            return LoopDecision(
                action="user_checkpoint",
                reasoning=f"Diminishing returns detected: improvement rate {improvement_rate:.1f}% < {self.improvement_threshold}%",
                user_checkpoint_info={
                    "situation": "Low improvement rate",
                    "options": [
                        "Continue with current approach",
                        "Change strategy/focus",
                        "Stop and proceed to next phase"
                    ],
                    "recommendation": "Consider changing approach"
                },
                quality_metrics={
                    "overall_score": overall_score,
                    "red_flags_count": len(red_flags),
                    "improvement_rate": improvement_rate
                }
            )
        
        elif len(red_flags) > 3:
            return LoopDecision(
                action="user_checkpoint",
                reasoning=f"Too many red flags detected: {len(red_flags)} critical issues",
                user_checkpoint_info={
                    "situation": "Multiple critical issues",
                    "options": [
                        "Address red flags and continue",
                        "Lower quality standards",
                        "Stop and accept current quality"
                    ],
                    "recommendation": "Address critical issues first"
                },
                quality_metrics={
                    "overall_score": overall_score,
                    "red_flags_count": len(red_flags),
                    "improvement_rate": improvement_rate
                }
            )
        
        else:
            # Continue with improvements
            next_instructions = self._generate_next_round_instructions(
                task, quality_scores, red_flags
            )
            
            return LoopDecision(
                action="continue",
                reasoning=f"Continue improving: score {overall_score:.2f} < {self.quality_threshold}, can improve further",
                next_round_instructions=next_instructions,
                quality_metrics={
                    "overall_score": overall_score,
                    "red_flags_count": len(red_flags),
                    "improvement_rate": improvement_rate
                }
            )
    
    def _calculate_improvement_rate(self) -> Optional[float]:
        """T√≠nh t·ª∑ l·ªá c·∫£i thi·ªán so v·ªõi v√≤ng tr∆∞·ªõc"""
        
        if len(self.iteration_scores) < 2:
            return None
        
        current_score = self.iteration_scores[-1]
        previous_score = self.iteration_scores[-2]
        
        if previous_score == 0:
            return None
        
        improvement = ((current_score - previous_score) / previous_score) * 100
        return improvement
    
    def _generate_next_round_instructions(self, 
                                        task: SynthesisTask,
                                        quality_scores: List[QualityScore],
                                        red_flags: List[QualityScore]) -> Dict[str, Any]:
        """T·∫°o h∆∞·ªõng d·∫´n cho v√≤ng ti·∫øp theo"""
        
        instructions = {
            "primary_llm": {
                "priority_actions": [],
                "specific_improvements": [],
                "questions_to_address": [],
                "diversity_actions": []
            },
            "ct_llm": {
                "focus_areas": [],
                "deep_dive_topics": []
            },
            "ae_llm": {
                "attack_vectors": [],
                "role_priority": []
            }
        }
        
        # Analyze weak areas
        weak_criteria = [score for score in quality_scores if score.raw_score < 6.0]
        
        for weak_criterion in weak_criteria:
            criterion = weak_criterion.criterion
            
            if criterion in ["tinh_logic", "nhat_quan"]:
                instructions["primary_llm"]["specific_improvements"].append(
                    f"Improve {criterion}: ensure logical consistency"
                )
                instructions["ct_llm"]["focus_areas"].append(criterion)
            
            elif criterion in ["tinh_kha_thi", "mo_hinh_kinh_doanh"]:
                instructions["ae_llm"]["attack_vectors"].append(
                    f"Challenge {criterion} assumptions"
                )
                instructions["ae_llm"]["role_priority"].append("VC")
        
        # Add red flag specific instructions
        for red_flag in red_flags:
            instructions["primary_llm"]["priority_actions"].append(
                f"URGENT: Address {red_flag.criterion} (score: {red_flag.raw_score})"
            )

        # N·∫øu ·ªü phase ideas, th√™m ch·ªâ d·∫´n ƒëa d·∫°ng h√≥a d·ª±a tr√™n diversity score
        if task.phase == "ideas":
            diversity = agent_input.context.get("idea_diversity_analysis", {})
            div_score = float(diversity.get("diversity_score", 1.0))
            duplicates = diversity.get("duplicates", [])
            if div_score < 0.7:
                instructions["primary_llm"]["diversity_actions"].append(
                    "T·∫°o th√™m 2 √Ω t∆∞·ªüng kh√°c bi·ªát r√µ r·ªát v·ªÅ kh√°ch h√†ng m·ª•c ti√™u ho·∫∑c m√¥ h√¨nh kinh doanh."
                )
            if duplicates:
                instructions["primary_llm"]["diversity_actions"].append(
                    f"T√°ch/g·ªôp ho·∫∑c ch·ªânh s·ª≠a c√°c c·∫∑p √Ω t∆∞·ªüng tr√πng l·∫∑p: {duplicates[:3]}"
                )
        
        return instructions
    
    async def validate_input(self, agent_input: AgentInput) -> bool:
        """Validate input cho Synthesis Assessment Agent"""
        
        if not await super().validate_input(agent_input):
            return False
        
        data = agent_input.data
        
        if isinstance(data, SynthesisTask):
            if not data.primary_output.strip():
                logger.error("SynthesisTask must have primary_output")
                return False
            if data.phase not in ["analysis", "ideas"]:
                logger.error("SynthesisTask phase must be 'analysis' or 'ideas'")
                return False
        
        return True
    
    async def post_process_output(self, llm_response, agent_input: AgentInput) -> str:
        """Post-process output t·ª´ LLM"""
        
        content = llm_response.content
        
        # Add comprehensive metadata
        task = agent_input.data if isinstance(agent_input.data, SynthesisTask) else None
        
        metadata_header = f"""
<!-- METADATA -->
Agent: SynthesisAssessmentAgent
Phase: {task.phase if task else 'unknown'}
Iteration: {agent_input.iteration}
Timestamp: {agent_input.metadata.get('timestamp', 'unknown')}
Overall_Score: {self.iteration_scores[-1] if self.iteration_scores else 'N/A'}
Decision: {self.iteration_decisions[-1].action if self.iteration_decisions else 'N/A'}
Synthesis_Mode: Comprehensive_Assessment
<!-- END METADATA -->

"""
        
        return metadata_header + content
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """L·∫•y t√≥m t·∫Øt performance qua c√°c v√≤ng l·∫∑p"""
        
        return {
            "total_iterations": len(self.iteration_scores),
            "scores_progression": self.iteration_scores,
            "final_score": self.iteration_scores[-1] if self.iteration_scores else 0,
            "best_score": max(self.iteration_scores) if self.iteration_scores else 0,
            "improvement_trend": self._calculate_improvement_rate(),
            "decisions_made": [d.action for d in self.iteration_decisions],
            "quality_achieved": self.iteration_scores[-1] >= self.quality_threshold if self.iteration_scores else False
        }
    
    def reset_tracking(self):
        """Reset tracking metrics"""
        self.iteration_scores = []
        self.iteration_decisions = []
        logger.info("Reset SynthesisAssessmentAgent tracking metrics")

# Helper functions
def create_synthesis_task(primary_output: str,
                         phase: str = "analysis",
                         ct_feedback: Optional[str] = None,
                         ae_feedback: Optional[str] = None,
                         esv_results: Optional[Dict[str, Any]] = None,
                         iteration: int = 1) -> SynthesisTask:
    """Helper ƒë·ªÉ t·∫°o SynthesisTask"""
    return SynthesisTask(
        primary_output=primary_output,
        ct_feedback=ct_feedback,
        ae_feedback=ae_feedback,
        esv_results=esv_results,
        phase=phase,
        iteration=iteration
    )
