"""
Dynamic Processor - Xá»­ lÃ½ dynamic input tá»« user
"""

import asyncio
import logging
import json
import re
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

from backend.core.llm_client import LLMClient, LLMMessage, LLMResponse
from backend.core.mcts_orchestrator import MCTSOrchestrator, MCTSSession
from backend.config import MCTSConfig, DEFAULT_CONFIG

logger = logging.getLogger(__name__)

class InputType(Enum):
    """Loáº¡i input tá»« user"""
    QUESTION = "question"
    ANALYSIS_REQUEST = "analysis_request"
    IDEA_REQUEST = "idea_request"
    DATA_TO_ANALYZE = "data_to_analyze"
    FEEDBACK = "feedback"
    GENERAL_CHAT = "general_chat"

@dataclass
class DynamicInput:
    """Dynamic input tá»« user"""
    content: str
    input_type: InputType
    context: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class ProcessedResponse:
    """Response Ä‘Ã£ xá»­ lÃ½"""
    content: str
    response_type: str
    success: bool
    suggestions: List[str] = field(default_factory=list)
    follow_up_questions: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

class DynamicProcessor:
    """
    Processor chÃ­nh Ä‘á»ƒ xá»­ lÃ½ má»i loáº¡i input tá»« user
    """
    
    def __init__(self, config: MCTSConfig = None):
        self.config = config or DEFAULT_CONFIG
        self.llm_client: Optional[LLMClient] = None
        self.mcts_orchestrator: Optional[MCTSOrchestrator] = None
        
        # Conversation context
        self.conversation_history: List[Dict[str, Any]] = []
        self.current_session: Optional[MCTSSession] = None
        
        # Classification prompts
        self.classifier_prompt = """
Báº¡n lÃ  má»™t AI classifier thÃ´ng minh. HÃ£y phÃ¢n loáº¡i input sau cá»§a user vÃ o má»™t trong cÃ¡c loáº¡i:

1. QUESTION - CÃ¢u há»i tá»•ng quÃ¡t vá» startup, business, technology
2. ANALYSIS_REQUEST - YÃªu cáº§u phÃ¢n tÃ­ch dá»¯ liá»‡u/thÃ´ng tin cá»¥ thá»ƒ  
3. IDEA_REQUEST - YÃªu cáº§u táº¡o Ã½ tÆ°á»Ÿng startup/business
4. DATA_TO_ANALYZE - Cung cáº¥p dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch
5. FEEDBACK - Feedback vá» káº¿t quáº£ trÆ°á»›c Ä‘Ã³
6. GENERAL_CHAT - TrÃ² chuyá»‡n thÃ´ng thÆ°á»ng

Input: "{input_text}"

Tráº£ vá» JSON format:
{{
    "type": "loáº¡i_Ä‘Æ°á»£c_phÃ¢n_loáº¡i",
    "confidence": 0.95,
    "reasoning": "lÃ½ do phÃ¢n loáº¡i",
    "extracted_intent": "Ã½ Ä‘á»‹nh chÃ­nh cá»§a user",
    "key_entities": ["entity1", "entity2"]
}}
"""
        
    async def __aenter__(self):
        """Async context manager entry"""
        await self.initialize()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.cleanup()
    
    async def initialize(self):
        """Khá»Ÿi táº¡o components"""
        logger.info("Initializing Dynamic Processor...")
        
        # Initialize LLM client
        self.llm_client = LLMClient(self.config.llm)
        await self.llm_client.start_session()
        
        # Initialize MCTS orchestrator
        self.mcts_orchestrator = MCTSOrchestrator(self.config)
        await self.mcts_orchestrator.initialize()
        
        logger.info("âœ… Dynamic Processor initialized successfully")
    
    async def cleanup(self):
        """Cleanup resources"""
        if self.llm_client:
            await self.llm_client.close_session()
        
        if self.mcts_orchestrator:
            await self.mcts_orchestrator.cleanup()
    
    async def process_user_input(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> ProcessedResponse:
        """
        Xá»­ lÃ½ input tá»« user má»™t cÃ¡ch dynamic
        
        Args:
            user_input: Input text tá»« user
            context: Context bá»• sung (optional)
        
        Returns:
            ProcessedResponse vá»›i káº¿t quáº£
        """
        
        try:
            # Step 1: Classify input
            classified_input = await self._classify_input(user_input, context)
            
            # Step 2: Process based on type
            response = await self._process_by_type(classified_input)
            
            # Step 3: Update conversation history
            self._update_conversation_history(user_input, response)
            
            return response
            
        except Exception as e:
            logger.error(f"Error processing user input: {str(e)}")
            return ProcessedResponse(
                content=f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}",
                response_type="error",
                success=False,
                suggestions=["HÃ£y thá»­ láº¡i vá»›i input khÃ¡c", "Kiá»ƒm tra káº¿t ná»‘i máº¡ng"]
            )
    
    async def _classify_input(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> DynamicInput:
        """PhÃ¢n loáº¡i input cá»§a user"""
        
        try:
            # Prepare classifier prompt
            prompt = self.classifier_prompt.format(input_text=user_input)
            
            # Add context if available
            if context:
                prompt += f"\n\nContext bá»• sung: {json.dumps(context, ensure_ascii=False)}"
            
            # Call LLM
            response = await self.llm_client.single_prompt(
                prompt=prompt,
                temperature=0.1  # Low temperature for consistent classification
            )
            
            if not response.success:
                # Fallback classification
                return self._fallback_classify(user_input, context)
            
            # Parse JSON response
            try:
                classification = json.loads(response.content)
                input_type = InputType(classification.get("type", "general_chat"))
                
                return DynamicInput(
                    content=user_input,
                    input_type=input_type,
                    context=context or {},
                    metadata={
                        "confidence": classification.get("confidence", 0.5),
                        "reasoning": classification.get("reasoning", ""),
                        "extracted_intent": classification.get("extracted_intent", ""),
                        "key_entities": classification.get("key_entities", [])
                    }
                )
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Failed to parse classification JSON: {str(e)}")
                return self._fallback_classify(user_input, context)
                
        except Exception as e:
            logger.error(f"Error in input classification: {str(e)}")
            return self._fallback_classify(user_input, context)
    
    def _fallback_classify(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> DynamicInput:
        """Fallback classification sá»­ dá»¥ng regex patterns"""
        
        user_lower = user_input.lower()
        
        # Question patterns
        question_patterns = [
            r'\b(táº¡i sao|lÃ m sao|nhÆ° tháº¿ nÃ o|lÃ  gÃ¬|cÃ³ pháº£i)\b',
            r'^\s*(cÃ³|báº¡n|ai|gÃ¬|á»Ÿ Ä‘Ã¢u|khi nÃ o)',
            r'\?'
        ]
        
        # Analysis request patterns
        analysis_patterns = [
            r'\b(phÃ¢n tÃ­ch|Ä‘Ã¡nh giÃ¡|nghiÃªn cá»©u|so sÃ¡nh)\b',
            r'\b(xu hÆ°á»›ng|thá»‹ trÆ°á»ng|dá»¯ liá»‡u|bÃ¡o cÃ¡o)\b',
            r'\b(insight|thá»‘ng kÃª|metric)\b'
        ]
        
        # Idea request patterns
        idea_patterns = [
            r'\b(Ã½ tÆ°á»Ÿng|idea|startup|sÃ¡ng táº¡o)\b',
            r'\b(táº¡o ra|Ä‘á» xuáº¥t|gá»£i Ã½|brainstorm)\b',
            r'\b(business|kinh doanh|dá»± Ã¡n)\b'
        ]
        
        # Data patterns
        data_patterns = [
            r'\b(Ä‘Ã¢y lÃ  dá»¯ liá»‡u|data|thÃ´ng tin|ná»™i dung)\b',
            r'\b(tá»« reddit|tá»« hackernews|tá»« product hunt)\b',
            r'^\s*[\{\[].*[\}\]]'  # JSON-like structure
        ]
        
        # Determine type
        if any(re.search(pattern, user_lower) for pattern in question_patterns):
            input_type = InputType.QUESTION
        elif any(re.search(pattern, user_lower) for pattern in analysis_patterns):
            input_type = InputType.ANALYSIS_REQUEST
        elif any(re.search(pattern, user_lower) for pattern in idea_patterns):
            input_type = InputType.IDEA_REQUEST
        elif any(re.search(pattern, user_lower) for pattern in data_patterns):
            input_type = InputType.DATA_TO_ANALYZE
        else:
            input_type = InputType.GENERAL_CHAT
        
        return DynamicInput(
            content=user_input,
            input_type=input_type,
            context=context or {},
            metadata={"fallback_classification": True}
        )
    
    async def _process_by_type(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ input dá»±a trÃªn type Ä‘Ã£ classify"""
        
        if dynamic_input.input_type == InputType.QUESTION:
            return await self._handle_question(dynamic_input)
        
        elif dynamic_input.input_type == InputType.ANALYSIS_REQUEST:
            return await self._handle_analysis_request(dynamic_input)
        
        elif dynamic_input.input_type == InputType.IDEA_REQUEST:
            return await self._handle_idea_request(dynamic_input)
        
        elif dynamic_input.input_type == InputType.DATA_TO_ANALYZE:
            return await self._handle_data_analysis(dynamic_input)
        
        elif dynamic_input.input_type == InputType.FEEDBACK:
            return await self._handle_feedback(dynamic_input)
        
        else:  # GENERAL_CHAT
            return await self._handle_general_chat(dynamic_input)
    
    async def _handle_question(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ cÃ¢u há»i tá»•ng quÃ¡t"""
        
        system_prompt = """
Báº¡n lÃ  má»™t chuyÃªn gia startup vÃ  business strategy. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a user má»™t cÃ¡ch:
- ChÃ­nh xÃ¡c vÃ  cÃ³ cÄƒn cá»©
- Practical vÃ  actionable  
- Cung cáº¥p examples cá»¥ thá»ƒ
- ÄÆ°a ra follow-up questions Ä‘á»ƒ hiá»ƒu sÃ¢u hÆ¡n

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.
"""
        
        response = await self.llm_client.single_prompt(
            prompt=dynamic_input.content,
            system_message=system_prompt,
            temperature=0.7
        )
        
        if not response.success:
            return ProcessedResponse(
                content="Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y lÃºc nÃ y.",
                response_type="error",
                success=False
            )
        
        # Extract follow-up questions tá»« response
        follow_ups = self._extract_follow_up_questions(response.content)
        
        return ProcessedResponse(
            content=response.content,
            response_type="answer",
            success=True,
            follow_up_questions=follow_ups,
            suggestions=[
                "Báº¡n cÃ³ muá»‘n tÃ´i phÃ¢n tÃ­ch sÃ¢u hÆ¡n vá» chá»§ Ä‘á» nÃ y?",
                "CÃ³ váº¥n Ä‘á» gÃ¬ khÃ¡c báº¡n muá»‘n há»i?",
                "Báº¡n cÃ³ muá»‘n tÃ´i táº¡o Ã½ tÆ°á»Ÿng startup liÃªn quan?"
            ]
        )
    
    async def _handle_analysis_request(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ yÃªu cáº§u phÃ¢n tÃ­ch"""
        
        # Check if cÃ³ sufficient data Ä‘á»ƒ analyze
        if len(dynamic_input.content) < 100:
            return ProcessedResponse(
                content="""
Äá»ƒ thá»±c hiá»‡n phÃ¢n tÃ­ch cháº¥t lÆ°á»£ng, tÃ´i cáº§n thÃªm thÃ´ng tin:

1. **Dá»¯ liá»‡u cá»¥ thá»ƒ**: Cung cáº¥p data sources, links, hoáº·c ná»™i dung Ä‘á»ƒ phÃ¢n tÃ­ch
2. **Scope phÃ¢n tÃ­ch**: Báº¡n muá»‘n phÃ¢n tÃ­ch aspect nÃ o? (market trends, competitors, user feedback, etc.)
3. **Má»¥c tiÃªu**: PhÃ¢n tÃ­ch nÃ y Ä‘á»ƒ lÃ m gÃ¬? (market research, idea validation, investment decision)

**VÃ­ dá»¥ input tá»‘t:**
- "PhÃ¢n tÃ­ch xu hÆ°á»›ng AI/ML startup tá»« dá»¯ liá»‡u Reddit r/MachineLearning trong thÃ¡ng 12/2024"
- "ÄÃ¡nh giÃ¡ thá»‹ trÆ°á»ng SaaS productivity tools dá»±a trÃªn ProductHunt launches gáº§n Ä‘Ã¢y"
""",
                response_type="request_more_info",
                success=False,
                suggestions=[
                    "Cung cáº¥p links hoáº·c raw data Ä‘á»ƒ phÃ¢n tÃ­ch",
                    "Specify lÄ©nh vá»±c cáº§n phÃ¢n tÃ­ch (AI, SaaS, Fintech, etc.)",
                    "Cho biáº¿t má»¥c tiÃªu cá»§a phÃ¢n tÃ­ch"
                ]
            )
        
        # Create synthetic data source tá»« user input
        data_sources = [{
            "type": "user_provided",
            "description": "Dá»¯ liá»‡u/yÃªu cáº§u phÃ¢n tÃ­ch tá»« user",
            "content": dynamic_input.content,
            "metadata": dynamic_input.metadata
        }]
        
        # Extract focus areas tá»« input
        focus_areas = self._extract_focus_areas(dynamic_input.content)
        
        # Set timeframe
        timeframe = {
            "start": "2024-01-01",
            "end": datetime.now().strftime("%Y-%m-%d")
        }
        
        try:
            # Run MCTS analysis
            session = await self.mcts_orchestrator.run_full_analysis(
                data_sources=data_sources,
                timeframe=timeframe,
                focus_areas=focus_areas
            )
            
            self.current_session = session
            
            # Format response
            analysis_summary = self._format_analysis_summary(session)
            
            return ProcessedResponse(
                content=analysis_summary,
                response_type="analysis_result",
                success=True,
                suggestions=[
                    "Báº¡n cÃ³ muá»‘n tÃ´i táº¡o Ã½ tÆ°á»Ÿng startup tá»« phÃ¢n tÃ­ch nÃ y?",
                    "Cáº§n tÃ´i phÃ¢n tÃ­ch sÃ¢u hÆ¡n vá» aspect nÃ o?",
                    "Muá»‘n export káº¿t quáº£ chi tiáº¿t khÃ´ng?"
                ],
                metadata={"session_id": session.session_id}
            )
            
        except Exception as e:
            logger.error(f"Analysis failed: {str(e)}")
            return ProcessedResponse(
                content=f"PhÃ¢n tÃ­ch gáº·p lá»—i: {str(e)}. HÃ£y thá»­ vá»›i input khÃ¡c hoáº·c Ä‘Æ¡n giáº£n hÃ³a yÃªu cáº§u.",
                response_type="error",
                success=False
            )
    
    async def _handle_idea_request(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ yÃªu cáº§u táº¡o Ã½ tÆ°á»Ÿng"""
        
        # Check if cÃ³ context Ä‘á»ƒ táº¡o Ã½ tÆ°á»Ÿng
        if not self.current_session or not self.current_session.analysis_results:
            # Generate ideas directly tá»« input
            return await self._generate_ideas_from_scratch(dynamic_input)
        
        # Use existing analysis Ä‘á»ƒ generate ideas
        return await self._generate_ideas_from_analysis(dynamic_input)
    
    async def _handle_data_analysis(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ dá»¯ liá»‡u cáº§n phÃ¢n tÃ­ch"""
        
        # Auto-trigger analysis vá»›i provided data
        analysis_input = DynamicInput(
            content=f"PhÃ¢n tÃ­ch dá»¯ liá»‡u sau:\n\n{dynamic_input.content}",
            input_type=InputType.ANALYSIS_REQUEST,
            context=dynamic_input.context,
            metadata=dynamic_input.metadata
        )
        
        return await self._handle_analysis_request(analysis_input)
    
    async def _handle_feedback(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ feedback tá»« user"""
        
        if not self.current_session:
            return ProcessedResponse(
                content="TÃ´i chÆ°a cÃ³ káº¿t quáº£ nÃ o Ä‘á»ƒ receive feedback. HÃ£y báº¯t Ä‘áº§u vá»›i má»™t phÃ¢n tÃ­ch hoáº·c yÃªu cáº§u Ã½ tÆ°á»Ÿng.",
                response_type="no_context",
                success=False
            )
        
        # Process feedback vÃ  improve
        system_prompt = """
Báº¡n nháº­n Ä‘Æ°á»£c feedback tá»« user vá» káº¿t quáº£ trÆ°á»›c Ä‘Ã³. HÃ£y:
1. Acknowledge feedback
2. Explain cÃ¡ch sáº½ cáº£i thiá»‡n
3. ÄÆ°a ra next steps cá»¥ thá»ƒ

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.
"""
        
        prompt = f"""
Feedback tá»« user: {dynamic_input.content}

Context: User Ä‘Ã£ nháº­n káº¿t quáº£ tá»« session {self.current_session.session_id}
"""
        
        response = await self.llm_client.single_prompt(
            prompt=prompt,
            system_message=system_prompt,
            temperature=0.6
        )
        
        return ProcessedResponse(
            content=response.content if response.success else "Cáº£m Æ¡n feedback cá»§a báº¡n. TÃ´i sáº½ cáº£i thiá»‡n trong láº§n tiáº¿p theo.",
            response_type="feedback_acknowledgment",
            success=response.success,
            suggestions=[
                "Muá»‘n tÃ´i re-run analysis vá»›i approach khÃ¡c?",
                "Cáº§n adjust tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡?",
                "CÃ³ specific aspect nÃ o cáº§n focus hÆ¡n?"
            ]
        )
    
    async def _handle_general_chat(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Xá»­ lÃ½ general chat"""
        
        system_prompt = """
Báº¡n lÃ  MCTS AI Assistant - chuyÃªn gia vá» startup analysis vÃ  idea generation. 
HÃ£y trÃ² chuyá»‡n thÃ¢n thiá»‡n vÃ  hÆ°á»›ng dáº«n user sá»­ dá»¥ng kháº£ nÄƒng cá»§a báº¡n.

Capabilities chÃ­nh:
- PhÃ¢n tÃ­ch dá»¯ liá»‡u startup/market trends
- Táº¡o Ã½ tÆ°á»Ÿng startup validated
- ÄÃ¡nh giÃ¡ business ideas
- Market research automation

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.
"""
        
        response = await self.llm_client.single_prompt(
            prompt=dynamic_input.content,
            system_message=system_prompt,
            temperature=0.8
        )
        
        return ProcessedResponse(
            content=response.content if response.success else "Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp báº¡n phÃ¢n tÃ­ch startup trends vÃ  táº¡o Ã½ tÆ°á»Ÿng business. Báº¡n cáº§n há»— trá»£ gÃ¬?",
            response_type="chat",
            success=response.success,
            suggestions=[
                "Há»i tÃ´i vá» startup trends",
                "YÃªu cáº§u phÃ¢n tÃ­ch market data",
                "Táº¡o Ã½ tÆ°á»Ÿng startup má»›i",
                "ÄÃ¡nh giÃ¡ business idea cá»§a báº¡n"
            ]
        )
    
    async def _generate_ideas_from_scratch(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Táº¡o Ã½ tÆ°á»Ÿng tá»« scratch (khÃ´ng cÃ³ analysis trÆ°á»›c)"""
        
        system_prompt = """
Báº¡n lÃ  chuyÃªn gia startup consultant. User yÃªu cáº§u táº¡o Ã½ tÆ°á»Ÿng startup.
HÃ£y táº¡o 3-5 Ã½ tÆ°á»Ÿng cháº¥t lÆ°á»£ng cao vá»›i:

1. Problem definition rÃµ rÃ ng
2. Solution overview
3. Target market
4. Business model basics
5. Competitive advantage
6. Implementation roadmap sÆ¡ bá»™

Format response theo structure:

# Ã TÆ¯á»NG STARTUP

## 1. [TÃªn Ã½ tÆ°á»Ÿng]
**Problem:** 
**Solution:**
**Target Market:**
**Business Model:**
**Competitive Advantage:**
**Next Steps:**

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.
"""
        
        prompt = f"YÃªu cáº§u táº¡o Ã½ tÆ°á»Ÿng: {dynamic_input.content}"
        
        response = await self.llm_client.single_prompt(
            prompt=prompt,
            system_message=system_prompt,
            temperature=0.7
        )
        
        return ProcessedResponse(
            content=response.content if response.success else "KhÃ´ng thá»ƒ táº¡o Ã½ tÆ°á»Ÿng lÃºc nÃ y. HÃ£y thá»­ láº¡i sau.",
            response_type="ideas_generated",
            success=response.success,
            suggestions=[
                "Muá»‘n tÃ´i phÃ¢n tÃ­ch chi tiáº¿t hÆ¡n Ã½ tÆ°á»Ÿng nÃ o?",
                "Cáº§n validate Ã½ tÆ°á»Ÿng vá»›i market data?",
                "Táº¡o business plan cho Ã½ tÆ°á»Ÿng cá»¥ thá»ƒ?"
            ]
        )
    
    async def _generate_ideas_from_analysis(self, dynamic_input: DynamicInput) -> ProcessedResponse:
        """Táº¡o Ã½ tÆ°á»Ÿng dá»±a trÃªn analysis cÃ³ sáºµn"""
        
        # Use MCTS idea generation
        try:
            # Continue with ideas phase
            self.mcts_orchestrator.session = self.current_session
            await self.mcts_orchestrator._run_ideas_phase()
            
            ideas_summary = self._format_ideas_summary(self.current_session)
            
            return ProcessedResponse(
                content=ideas_summary,
                response_type="validated_ideas",
                success=True,
                suggestions=[
                    "PhÃ¢n tÃ­ch deeper vÃ o Ã½ tÆ°á»Ÿng nÃ o?",
                    "Táº¡o business plan chi tiáº¿t?",
                    "Export full report?"
                ],
                metadata={"session_id": self.current_session.session_id}
            )
            
        except Exception as e:
            logger.error(f"Ideas generation failed: {str(e)}")
            return await self._generate_ideas_from_scratch(dynamic_input)
    
    def _extract_focus_areas(self, content: str) -> List[str]:
        """Extract focus areas tá»« user input"""
        
        content_lower = content.lower()
        
        # Common focus areas mapping
        focus_mapping = {
            "ai": ["AI/ML", "Artificial Intelligence"],
            "machine learning": ["AI/ML"],
            "ml": ["AI/ML"],
            "saas": ["SaaS", "Software"],
            "fintech": ["Fintech", "Finance"],
            "healthtech": ["HealthTech", "Healthcare"],
            "edtech": ["EdTech", "Education"],
            "ecommerce": ["E-commerce", "Retail"],
            "blockchain": ["Blockchain", "Web3"],
            "iot": ["IoT", "Hardware"],
            "mobile": ["Mobile Apps"],
            "web": ["Web Applications"],
            "productivity": ["Productivity Tools"],
            "marketing": ["Marketing Tools"],
            "hr": ["HR Tech"],
            "real estate": ["PropTech"]
        }
        
        detected_areas = []
        for keyword, areas in focus_mapping.items():
            if keyword in content_lower:
                detected_areas.extend(areas)
        
        # Remove duplicates vÃ  return
        return list(set(detected_areas)) if detected_areas else ["General Technology", "Market Analysis"]
    
    def _extract_follow_up_questions(self, content: str) -> List[str]:
        """Extract follow-up questions tá»« response"""
        
        # Look for questions in response
        question_pattern = r'([^.!?]*\?)'
        questions = re.findall(question_pattern, content)
        
        # Clean vÃ  filter
        follow_ups = []
        for q in questions[-3:]:  # Take last 3 questions
            clean_q = q.strip()
            if len(clean_q) > 10 and clean_q not in follow_ups:
                follow_ups.append(clean_q)
        
        return follow_ups
    
    def _format_analysis_summary(self, session: MCTSSession) -> str:
        """Format analysis results for user"""
        
        if not session.analysis_results:
            return "PhÃ¢n tÃ­ch chÆ°a hoÃ n thÃ nh."
        
        # Extract key insights tá»« analysis
        summary = f"""
# ğŸ“Š Káº¾T QUáº¢ PHÃ‚N TÃCH

**Session ID:** {session.session_id}
**Thá»i gian:** {session.start_time.strftime('%Y-%m-%d %H:%M')}
**Sá»‘ vÃ²ng láº·p:** {session.analysis_iteration}

## TÃ³m táº¯t chÃ­nh

{session.analysis_results[:1000]}{'...' if len(session.analysis_results) > 1000 else ''}

## Cháº¥t lÆ°á»£ng káº¿t quáº£

- **VÃ²ng láº·p hoÃ n thÃ nh:** {session.analysis_iteration}/{self.config.max_analysis_loops}
- **Status:** {session.current_phase.value}

ğŸ“„ **Káº¿t quáº£ Ä‘áº§y Ä‘á»§** Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ  cÃ³ thá»ƒ export náº¿u cáº§n.
"""
        
        return summary
    
    def _format_ideas_summary(self, session: MCTSSession) -> str:
        """Format ideas results for user"""
        
        if not session.ideas_results:
            return "ChÆ°a cÃ³ Ã½ tÆ°á»Ÿng Ä‘Æ°á»£c táº¡o."
        
        summary = f"""
# ğŸ’¡ Ã TÆ¯á»NG STARTUP ÄÃƒ VALIDATED

**Session ID:** {session.session_id}
**Dá»±a trÃªn phÃ¢n tÃ­ch:** {session.analysis_iteration} vÃ²ng láº·p
**Ã tÆ°á»Ÿng Ä‘Æ°á»£c test:** {session.ideas_iteration} vÃ²ng láº·p

## Top Ideas

{session.ideas_results[:1500]}{'...' if len(session.ideas_results) > 1500 else ''}

## Validation Summary

- **Analysis Phase:** {session.analysis_iteration} iterations
- **Ideas Phase:** {session.ideas_iteration} iterations  
- **Quality Assurance:** ÄÃ£ qua Critical Thinking + Adversarial Testing
- **External Validation:** {'âœ…' if self.config.enable_external_validation else 'âŒ'}

ğŸ“„ **Full report** vá»›i scoring chi tiáº¿t cÃ³ thá»ƒ Ä‘Æ°á»£c export.
"""
        
        return summary
    
    def _update_conversation_history(self, user_input: str, response: ProcessedResponse):
        """Update conversation history"""
        
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "response": response.__dict__,
            "session_id": self.current_session.session_id if self.current_session else None
        })
        
        # Keep only last 50 conversations
        if len(self.conversation_history) > 50:
            self.conversation_history = self.conversation_history[-50:]
    
    def get_conversation_context(self) -> Dict[str, Any]:
        """Láº¥y context cá»§a conversation hiá»‡n táº¡i"""
        
        return {
            "conversation_length": len(self.conversation_history),
            "current_session": self.current_session.session_id if self.current_session else None,
            "last_interactions": self.conversation_history[-5:] if self.conversation_history else [],
            "active_topics": self._extract_active_topics()
        }
    
    def _extract_active_topics(self) -> List[str]:
        """Extract active topics tá»« conversation history"""
        
        if not self.conversation_history:
            return []
        
        # Simple keyword extraction tá»« recent conversations
        recent_inputs = [conv["user_input"] for conv in self.conversation_history[-10:]]
        combined_text = " ".join(recent_inputs).lower()
        
        # Common startup/business topics
        topics = {
            "startup": "Startup Strategy",
            "business": "Business Development", 
            "market": "Market Analysis",
            "competitor": "Competitive Analysis",
            "funding": "Funding & Investment",
            "technology": "Technology Trends",
            "product": "Product Development",
            "user": "User Experience"
        }
        
        active = []
        for keyword, topic in topics.items():
            if keyword in combined_text:
                active.append(topic)
        
        return active[:3]  # Return top 3 active topics

# Convenience functions
async def process_user_message(message: str, config: MCTSConfig = None) -> ProcessedResponse:
    """Quick function Ä‘á»ƒ process user message"""
    
    async with DynamicProcessor(config) as processor:
        return await processor.process_user_input(message)

async def start_interactive_session(config: MCTSConfig = None):
    """Start interactive session vá»›i user"""
    
    from rich.console import Console
    from rich.panel import Panel
    
    console = Console()
    
    console.print(Panel.fit(
        "ğŸ§  MCTS Interactive Session\nNháº­p 'quit' Ä‘á»ƒ thoÃ¡t",
        title="Dynamic AI Assistant",
        border_style="blue"
    ))
    
    async with DynamicProcessor(config) as processor:
        while True:
            try:
                # Get user input
                user_input = input("\nğŸ’¬ Báº¡n: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    console.print("ğŸ‘‹ Táº¡m biá»‡t! Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng MCTS.", style="green")
                    break
                
                if not user_input:
                    continue
                
                # Process input
                console.print("\nğŸ¤” Äang xá»­ lÃ½...", style="yellow")
                response = await processor.process_user_input(user_input)
                
                # Display response
                console.print(f"\nğŸ¤– MCTS: {response.content}", style="white")
                
                # Show suggestions if available
                if response.suggestions:
                    console.print("\nğŸ’¡ Gá»£i Ã½:", style="cyan")
                    for i, suggestion in enumerate(response.suggestions, 1):
                        console.print(f"   {i}. {suggestion}", style="dim cyan")
                
                # Show follow-up questions if available
                if response.follow_up_questions:
                    console.print("\nâ“ CÃ¢u há»i tiáº¿p theo:", style="magenta")
                    for i, question in enumerate(response.follow_up_questions, 1):
                        console.print(f"   {i}. {question}", style="dim magenta")
                
            except KeyboardInterrupt:
                console.print("\nğŸ‘‹ Táº¡m biá»‡t!", style="green")
                break
            except Exception as e:
                console.print(f"\nâŒ Lá»—i: {str(e)}", style="red")

if __name__ == "__main__":
    # Test dynamic processor
    import asyncio
    
    async def test():
        test_inputs = [
            "AI startup trends trong nÄƒm 2024?",
            "TÃ´i cÃ³ data tá»« Reddit vá» SaaS tools, cÃ³ thá»ƒ phÃ¢n tÃ­ch khÃ´ng?",
            "Táº¡o Ã½ tÆ°á»Ÿng startup trong lÄ©nh vá»±c fintech",
            "Xin chÃ o, báº¡n cÃ³ thá»ƒ lÃ m gÃ¬?"
        ]
        
        for test_input in test_inputs:
            print(f"\nInput: {test_input}")
            response = await process_user_message(test_input)
            print(f"Response: {response.content[:200]}...")
            print(f"Type: {response.response_type}")
    
    asyncio.run(test())
